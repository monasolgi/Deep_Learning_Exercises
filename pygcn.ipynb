{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import __future__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "\n",
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "\n",
    "def load_data(path=\"../data/cora/\", dataset=\"cora\"):\n",
    "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
    "                                        dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
    "                                    dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    features = normalize(features)\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test\n",
    "\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from layers import GraphConvolution\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#layers\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "\n",
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ') '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n",
      "Epoch: 0001 loss_train: 1.9709 acc_train: 0.1357 loss_val: 1.9517 acc_val: 0.1167 time: 1.2968s\n",
      "Epoch: 0002 loss_train: 1.9542 acc_train: 0.1071 loss_val: 1.9395 acc_val: 0.1200 time: 0.0781s\n",
      "Epoch: 0003 loss_train: 1.9433 acc_train: 0.1071 loss_val: 1.9265 acc_val: 0.1233 time: 0.0781s\n",
      "Epoch: 0004 loss_train: 1.9263 acc_train: 0.1214 loss_val: 1.9135 acc_val: 0.1267 time: 0.0803s\n",
      "Epoch: 0005 loss_train: 1.9152 acc_train: 0.1429 loss_val: 1.9004 acc_val: 0.3667 time: 0.0849s\n",
      "Epoch: 0006 loss_train: 1.9009 acc_train: 0.3000 loss_val: 1.8872 acc_val: 0.3500 time: 0.0627s\n",
      "Epoch: 0007 loss_train: 1.8800 acc_train: 0.3214 loss_val: 1.8742 acc_val: 0.3500 time: 0.0935s\n",
      "Epoch: 0008 loss_train: 1.8672 acc_train: 0.3286 loss_val: 1.8614 acc_val: 0.3500 time: 0.0630s\n",
      "Epoch: 0009 loss_train: 1.8578 acc_train: 0.3143 loss_val: 1.8489 acc_val: 0.3500 time: 0.0781s\n",
      "Epoch: 0010 loss_train: 1.8476 acc_train: 0.3214 loss_val: 1.8369 acc_val: 0.3500 time: 0.0821s\n",
      "Epoch: 0011 loss_train: 1.8357 acc_train: 0.3000 loss_val: 1.8252 acc_val: 0.3500 time: 0.0709s\n",
      "Epoch: 0012 loss_train: 1.8194 acc_train: 0.2786 loss_val: 1.8141 acc_val: 0.3500 time: 0.0781s\n",
      "Epoch: 0013 loss_train: 1.8196 acc_train: 0.3071 loss_val: 1.8037 acc_val: 0.3500 time: 0.0868s\n",
      "Epoch: 0014 loss_train: 1.7896 acc_train: 0.3214 loss_val: 1.7940 acc_val: 0.3500 time: 0.0673s\n",
      "Epoch: 0015 loss_train: 1.8030 acc_train: 0.2929 loss_val: 1.7851 acc_val: 0.3500 time: 0.0781s\n",
      "Epoch: 0016 loss_train: 1.7879 acc_train: 0.2929 loss_val: 1.7768 acc_val: 0.3500 time: 0.0898s\n",
      "Epoch: 0017 loss_train: 1.7730 acc_train: 0.3071 loss_val: 1.7692 acc_val: 0.3500 time: 0.0647s\n",
      "Epoch: 0018 loss_train: 1.7588 acc_train: 0.3000 loss_val: 1.7622 acc_val: 0.3500 time: 0.0781s\n",
      "Epoch: 0019 loss_train: 1.7607 acc_train: 0.3286 loss_val: 1.7558 acc_val: 0.3500 time: 0.0937s\n",
      "Epoch: 0020 loss_train: 1.7593 acc_train: 0.2929 loss_val: 1.7496 acc_val: 0.3500 time: 0.0673s\n",
      "Epoch: 0021 loss_train: 1.7384 acc_train: 0.2929 loss_val: 1.7434 acc_val: 0.3500 time: 0.0781s\n",
      "Epoch: 0022 loss_train: 1.7189 acc_train: 0.3214 loss_val: 1.7374 acc_val: 0.3500 time: 0.0868s\n",
      "Epoch: 0023 loss_train: 1.7371 acc_train: 0.3000 loss_val: 1.7315 acc_val: 0.3500 time: 0.0802s\n",
      "Epoch: 0024 loss_train: 1.7240 acc_train: 0.3143 loss_val: 1.7255 acc_val: 0.3500 time: 0.0781s\n",
      "Epoch: 0025 loss_train: 1.7020 acc_train: 0.3286 loss_val: 1.7195 acc_val: 0.3500 time: 0.0815s\n",
      "Epoch: 0026 loss_train: 1.7063 acc_train: 0.3000 loss_val: 1.7135 acc_val: 0.3500 time: 0.0650s\n",
      "Epoch: 0027 loss_train: 1.7010 acc_train: 0.3286 loss_val: 1.7074 acc_val: 0.3500 time: 0.0781s\n",
      "Epoch: 0028 loss_train: 1.7128 acc_train: 0.2857 loss_val: 1.7015 acc_val: 0.3500 time: 0.0797s\n",
      "Epoch: 0029 loss_train: 1.6714 acc_train: 0.3143 loss_val: 1.6955 acc_val: 0.3500 time: 0.0647s\n",
      "Epoch: 0030 loss_train: 1.6771 acc_train: 0.3357 loss_val: 1.6897 acc_val: 0.3500 time: 0.0781s\n",
      "Epoch: 0031 loss_train: 1.6791 acc_train: 0.3000 loss_val: 1.6839 acc_val: 0.3500 time: 0.0909s\n",
      "Epoch: 0032 loss_train: 1.6715 acc_train: 0.3000 loss_val: 1.6783 acc_val: 0.3500 time: 0.0703s\n",
      "Epoch: 0033 loss_train: 1.6560 acc_train: 0.3143 loss_val: 1.6728 acc_val: 0.3500 time: 0.0781s\n",
      "Epoch: 0034 loss_train: 1.6422 acc_train: 0.3000 loss_val: 1.6671 acc_val: 0.3467 time: 0.0835s\n",
      "Epoch: 0035 loss_train: 1.6401 acc_train: 0.3071 loss_val: 1.6612 acc_val: 0.3467 time: 0.0717s\n",
      "Epoch: 0036 loss_train: 1.6189 acc_train: 0.3857 loss_val: 1.6550 acc_val: 0.3467 time: 0.0781s\n",
      "Epoch: 0037 loss_train: 1.5952 acc_train: 0.3714 loss_val: 1.6484 acc_val: 0.3467 time: 0.0847s\n",
      "Epoch: 0038 loss_train: 1.6128 acc_train: 0.3714 loss_val: 1.6413 acc_val: 0.3500 time: 0.0753s\n",
      "Epoch: 0039 loss_train: 1.5769 acc_train: 0.3857 loss_val: 1.6343 acc_val: 0.3600 time: 0.0781s\n",
      "Epoch: 0040 loss_train: 1.5509 acc_train: 0.4429 loss_val: 1.6270 acc_val: 0.3600 time: 0.0821s\n",
      "Epoch: 0041 loss_train: 1.5787 acc_train: 0.4071 loss_val: 1.6191 acc_val: 0.3600 time: 0.0766s\n",
      "Epoch: 0042 loss_train: 1.5811 acc_train: 0.3786 loss_val: 1.6110 acc_val: 0.3733 time: 0.0781s\n",
      "Epoch: 0043 loss_train: 1.5453 acc_train: 0.3929 loss_val: 1.6026 acc_val: 0.3733 time: 0.0801s\n",
      "Epoch: 0044 loss_train: 1.5174 acc_train: 0.4214 loss_val: 1.5942 acc_val: 0.3933 time: 0.0647s\n",
      "Epoch: 0045 loss_train: 1.5301 acc_train: 0.4357 loss_val: 1.5853 acc_val: 0.3933 time: 0.0625s\n",
      "Epoch: 0046 loss_train: 1.5232 acc_train: 0.4286 loss_val: 1.5761 acc_val: 0.4000 time: 0.0917s\n",
      "Epoch: 0047 loss_train: 1.5287 acc_train: 0.4000 loss_val: 1.5666 acc_val: 0.4033 time: 0.0640s\n",
      "Epoch: 0048 loss_train: 1.5185 acc_train: 0.4071 loss_val: 1.5567 acc_val: 0.4200 time: 0.0781s\n",
      "Epoch: 0049 loss_train: 1.4666 acc_train: 0.4500 loss_val: 1.5467 acc_val: 0.4267 time: 0.0904s\n",
      "Epoch: 0050 loss_train: 1.4538 acc_train: 0.5000 loss_val: 1.5367 acc_val: 0.4367 time: 0.0653s\n",
      "Epoch: 0051 loss_train: 1.4263 acc_train: 0.4714 loss_val: 1.5267 acc_val: 0.4500 time: 0.0781s\n",
      "Epoch: 0052 loss_train: 1.4222 acc_train: 0.4786 loss_val: 1.5168 acc_val: 0.4700 time: 0.0767s\n",
      "Epoch: 0053 loss_train: 1.4441 acc_train: 0.4929 loss_val: 1.5068 acc_val: 0.4800 time: 0.0700s\n",
      "Epoch: 0054 loss_train: 1.4262 acc_train: 0.4929 loss_val: 1.4968 acc_val: 0.4833 time: 0.0756s\n",
      "Epoch: 0055 loss_train: 1.3998 acc_train: 0.5571 loss_val: 1.4866 acc_val: 0.5167 time: 0.0922s\n",
      "Epoch: 0056 loss_train: 1.3707 acc_train: 0.5786 loss_val: 1.4762 acc_val: 0.5267 time: 0.0727s\n",
      "Epoch: 0057 loss_train: 1.3772 acc_train: 0.6143 loss_val: 1.4657 acc_val: 0.5567 time: 0.0781s\n",
      "Epoch: 0058 loss_train: 1.3612 acc_train: 0.5857 loss_val: 1.4549 acc_val: 0.5600 time: 0.0876s\n",
      "Epoch: 0059 loss_train: 1.3588 acc_train: 0.5929 loss_val: 1.4439 acc_val: 0.5733 time: 0.0723s\n",
      "Epoch: 0060 loss_train: 1.2941 acc_train: 0.6500 loss_val: 1.4327 acc_val: 0.5833 time: 0.0781s\n",
      "Epoch: 0061 loss_train: 1.3130 acc_train: 0.6571 loss_val: 1.4209 acc_val: 0.5900 time: 0.0825s\n",
      "Epoch: 0062 loss_train: 1.2808 acc_train: 0.6429 loss_val: 1.4088 acc_val: 0.6033 time: 0.0790s\n",
      "Epoch: 0063 loss_train: 1.2937 acc_train: 0.6214 loss_val: 1.3964 acc_val: 0.6200 time: 0.0625s\n",
      "Epoch: 0064 loss_train: 1.2632 acc_train: 0.6714 loss_val: 1.3839 acc_val: 0.6300 time: 0.0780s\n",
      "Epoch: 0065 loss_train: 1.2885 acc_train: 0.6643 loss_val: 1.3715 acc_val: 0.6467 time: 0.0743s\n",
      "Epoch: 0066 loss_train: 1.2230 acc_train: 0.6786 loss_val: 1.3591 acc_val: 0.6500 time: 0.0781s\n",
      "Epoch: 0067 loss_train: 1.2216 acc_train: 0.7000 loss_val: 1.3472 acc_val: 0.6633 time: 0.0821s\n",
      "Epoch: 0068 loss_train: 1.2047 acc_train: 0.6786 loss_val: 1.3353 acc_val: 0.6667 time: 0.0657s\n",
      "Epoch: 0069 loss_train: 1.1948 acc_train: 0.7286 loss_val: 1.3238 acc_val: 0.6667 time: 0.0781s\n",
      "Epoch: 0070 loss_train: 1.1518 acc_train: 0.7286 loss_val: 1.3122 acc_val: 0.6700 time: 0.0859s\n",
      "Epoch: 0071 loss_train: 1.1332 acc_train: 0.7000 loss_val: 1.3011 acc_val: 0.6867 time: 0.0676s\n",
      "Epoch: 0072 loss_train: 1.1570 acc_train: 0.7000 loss_val: 1.2905 acc_val: 0.6933 time: 0.0781s\n",
      "Epoch: 0073 loss_train: 1.1419 acc_train: 0.7500 loss_val: 1.2807 acc_val: 0.7033 time: 0.0921s\n",
      "Epoch: 0074 loss_train: 1.1309 acc_train: 0.7286 loss_val: 1.2712 acc_val: 0.7167 time: 0.0703s\n",
      "Epoch: 0075 loss_train: 1.1132 acc_train: 0.7357 loss_val: 1.2619 acc_val: 0.7200 time: 0.0781s\n",
      "Epoch: 0076 loss_train: 1.1490 acc_train: 0.7500 loss_val: 1.2512 acc_val: 0.7200 time: 0.0914s\n",
      "Epoch: 0077 loss_train: 1.0840 acc_train: 0.7643 loss_val: 1.2404 acc_val: 0.7233 time: 0.0709s\n",
      "Epoch: 0078 loss_train: 1.0657 acc_train: 0.7643 loss_val: 1.2294 acc_val: 0.7233 time: 0.0625s\n",
      "Epoch: 0079 loss_train: 1.0593 acc_train: 0.7429 loss_val: 1.2190 acc_val: 0.7267 time: 0.0905s\n",
      "Epoch: 0080 loss_train: 1.0471 acc_train: 0.7643 loss_val: 1.2088 acc_val: 0.7300 time: 0.0649s\n",
      "Epoch: 0081 loss_train: 1.0496 acc_train: 0.7571 loss_val: 1.1987 acc_val: 0.7400 time: 0.0625s\n",
      "Epoch: 0082 loss_train: 1.0089 acc_train: 0.7786 loss_val: 1.1893 acc_val: 0.7400 time: 0.0791s\n",
      "Epoch: 0083 loss_train: 1.0756 acc_train: 0.7429 loss_val: 1.1796 acc_val: 0.7400 time: 0.0723s\n",
      "Epoch: 0084 loss_train: 1.0116 acc_train: 0.7286 loss_val: 1.1699 acc_val: 0.7400 time: 0.0781s\n",
      "Epoch: 0085 loss_train: 0.9912 acc_train: 0.7786 loss_val: 1.1608 acc_val: 0.7400 time: 0.0880s\n",
      "Epoch: 0086 loss_train: 1.0179 acc_train: 0.7429 loss_val: 1.1517 acc_val: 0.7433 time: 0.0773s\n",
      "Epoch: 0087 loss_train: 0.9745 acc_train: 0.7929 loss_val: 1.1428 acc_val: 0.7367 time: 0.0625s\n",
      "Epoch: 0088 loss_train: 0.9883 acc_train: 0.7786 loss_val: 1.1342 acc_val: 0.7367 time: 0.0820s\n",
      "Epoch: 0089 loss_train: 0.9943 acc_train: 0.7500 loss_val: 1.1258 acc_val: 0.7400 time: 0.0757s\n",
      "Epoch: 0090 loss_train: 0.9311 acc_train: 0.7857 loss_val: 1.1173 acc_val: 0.7300 time: 0.0781s\n",
      "Epoch: 0091 loss_train: 0.9393 acc_train: 0.7786 loss_val: 1.1092 acc_val: 0.7333 time: 0.0848s\n",
      "Epoch: 0092 loss_train: 0.9219 acc_train: 0.7714 loss_val: 1.1016 acc_val: 0.7333 time: 0.0773s\n",
      "Epoch: 0093 loss_train: 0.9132 acc_train: 0.7786 loss_val: 1.0938 acc_val: 0.7267 time: 0.0781s\n",
      "Epoch: 0094 loss_train: 0.9516 acc_train: 0.7643 loss_val: 1.0853 acc_val: 0.7233 time: 0.0921s\n",
      "Epoch: 0095 loss_train: 0.9414 acc_train: 0.7429 loss_val: 1.0763 acc_val: 0.7300 time: 0.0689s\n",
      "Epoch: 0096 loss_train: 0.9016 acc_train: 0.8214 loss_val: 1.0665 acc_val: 0.7367 time: 0.0781s\n",
      "Epoch: 0097 loss_train: 0.8925 acc_train: 0.7714 loss_val: 1.0566 acc_val: 0.7433 time: 0.0911s\n",
      "Epoch: 0098 loss_train: 0.9136 acc_train: 0.7643 loss_val: 1.0473 acc_val: 0.7400 time: 0.0623s\n",
      "Epoch: 0099 loss_train: 0.8726 acc_train: 0.8071 loss_val: 1.0390 acc_val: 0.7467 time: 0.0781s\n",
      "Epoch: 0100 loss_train: 0.8719 acc_train: 0.7857 loss_val: 1.0312 acc_val: 0.7500 time: 0.0772s\n",
      "Epoch: 0101 loss_train: 0.8826 acc_train: 0.8000 loss_val: 1.0236 acc_val: 0.7567 time: 0.0684s\n",
      "Epoch: 0102 loss_train: 0.8362 acc_train: 0.8071 loss_val: 1.0164 acc_val: 0.7467 time: 0.0781s\n",
      "Epoch: 0103 loss_train: 0.7760 acc_train: 0.8714 loss_val: 1.0100 acc_val: 0.7567 time: 0.0902s\n",
      "Epoch: 0104 loss_train: 0.8352 acc_train: 0.8214 loss_val: 1.0043 acc_val: 0.7600 time: 0.0737s\n",
      "Epoch: 0105 loss_train: 0.8147 acc_train: 0.8500 loss_val: 0.9980 acc_val: 0.7633 time: 0.0781s\n",
      "Epoch: 0106 loss_train: 0.8296 acc_train: 0.8071 loss_val: 0.9911 acc_val: 0.7633 time: 0.0828s\n",
      "Epoch: 0107 loss_train: 0.7840 acc_train: 0.7929 loss_val: 0.9824 acc_val: 0.7733 time: 0.0772s\n",
      "Epoch: 0108 loss_train: 0.8373 acc_train: 0.7857 loss_val: 0.9734 acc_val: 0.7733 time: 0.0597s\n",
      "Epoch: 0109 loss_train: 0.7864 acc_train: 0.8286 loss_val: 0.9651 acc_val: 0.7800 time: 0.0945s\n",
      "Epoch: 0110 loss_train: 0.7923 acc_train: 0.8357 loss_val: 0.9572 acc_val: 0.7800 time: 0.0703s\n",
      "Epoch: 0111 loss_train: 0.7559 acc_train: 0.8500 loss_val: 0.9505 acc_val: 0.7833 time: 0.0781s\n",
      "Epoch: 0112 loss_train: 0.7357 acc_train: 0.8429 loss_val: 0.9439 acc_val: 0.7867 time: 0.0864s\n",
      "Epoch: 0113 loss_train: 0.8002 acc_train: 0.8286 loss_val: 0.9379 acc_val: 0.7833 time: 0.0739s\n",
      "Epoch: 0114 loss_train: 0.7577 acc_train: 0.8357 loss_val: 0.9321 acc_val: 0.7833 time: 0.0781s\n",
      "Epoch: 0115 loss_train: 0.7495 acc_train: 0.8286 loss_val: 0.9280 acc_val: 0.7867 time: 0.0868s\n",
      "Epoch: 0116 loss_train: 0.7477 acc_train: 0.8643 loss_val: 0.9232 acc_val: 0.7833 time: 0.0765s\n",
      "Epoch: 0117 loss_train: 0.7169 acc_train: 0.8357 loss_val: 0.9187 acc_val: 0.7833 time: 0.0781s\n",
      "Epoch: 0118 loss_train: 0.7431 acc_train: 0.8571 loss_val: 0.9129 acc_val: 0.7833 time: 0.0827s\n",
      "Epoch: 0119 loss_train: 0.6957 acc_train: 0.8714 loss_val: 0.9073 acc_val: 0.7833 time: 0.0656s\n",
      "Epoch: 0120 loss_train: 0.6827 acc_train: 0.8857 loss_val: 0.9019 acc_val: 0.7833 time: 0.0781s\n",
      "Epoch: 0121 loss_train: 0.6995 acc_train: 0.8571 loss_val: 0.8963 acc_val: 0.7833 time: 0.0782s\n",
      "Epoch: 0122 loss_train: 0.7107 acc_train: 0.8429 loss_val: 0.8910 acc_val: 0.7867 time: 0.0673s\n",
      "Epoch: 0123 loss_train: 0.6648 acc_train: 0.8500 loss_val: 0.8864 acc_val: 0.7867 time: 0.0781s\n",
      "Epoch: 0124 loss_train: 0.6815 acc_train: 0.8571 loss_val: 0.8830 acc_val: 0.7900 time: 0.0929s\n",
      "Epoch: 0125 loss_train: 0.6828 acc_train: 0.8643 loss_val: 0.8799 acc_val: 0.7900 time: 0.0781s\n",
      "Epoch: 0126 loss_train: 0.6137 acc_train: 0.8786 loss_val: 0.8769 acc_val: 0.7867 time: 0.0781s\n",
      "Epoch: 0127 loss_train: 0.6517 acc_train: 0.8714 loss_val: 0.8741 acc_val: 0.7833 time: 0.0817s\n",
      "Epoch: 0128 loss_train: 0.6792 acc_train: 0.8714 loss_val: 0.8708 acc_val: 0.7833 time: 0.0673s\n",
      "Epoch: 0129 loss_train: 0.6489 acc_train: 0.8643 loss_val: 0.8662 acc_val: 0.7833 time: 0.0781s\n",
      "Epoch: 0130 loss_train: 0.6343 acc_train: 0.8857 loss_val: 0.8612 acc_val: 0.7867 time: 0.0910s\n",
      "Epoch: 0131 loss_train: 0.6655 acc_train: 0.8786 loss_val: 0.8562 acc_val: 0.7867 time: 0.0729s\n",
      "Epoch: 0132 loss_train: 0.6506 acc_train: 0.8643 loss_val: 0.8513 acc_val: 0.7900 time: 0.0781s\n",
      "Epoch: 0133 loss_train: 0.6672 acc_train: 0.8500 loss_val: 0.8472 acc_val: 0.7900 time: 0.0847s\n",
      "Epoch: 0134 loss_train: 0.6395 acc_train: 0.8571 loss_val: 0.8436 acc_val: 0.7900 time: 0.0765s\n",
      "Epoch: 0135 loss_train: 0.6039 acc_train: 0.8643 loss_val: 0.8406 acc_val: 0.7900 time: 0.0781s\n",
      "Epoch: 0136 loss_train: 0.6224 acc_train: 0.8929 loss_val: 0.8373 acc_val: 0.7933 time: 0.0824s\n",
      "Epoch: 0137 loss_train: 0.6006 acc_train: 0.9000 loss_val: 0.8341 acc_val: 0.7900 time: 0.0723s\n",
      "Epoch: 0138 loss_train: 0.6074 acc_train: 0.8714 loss_val: 0.8308 acc_val: 0.7900 time: 0.0781s\n",
      "Epoch: 0139 loss_train: 0.6417 acc_train: 0.8786 loss_val: 0.8275 acc_val: 0.7900 time: 0.0861s\n",
      "Epoch: 0140 loss_train: 0.6527 acc_train: 0.8786 loss_val: 0.8239 acc_val: 0.7900 time: 0.0673s\n",
      "Epoch: 0141 loss_train: 0.5830 acc_train: 0.8643 loss_val: 0.8211 acc_val: 0.7900 time: 0.0781s\n",
      "Epoch: 0142 loss_train: 0.6035 acc_train: 0.8929 loss_val: 0.8190 acc_val: 0.7867 time: 0.0872s\n",
      "Epoch: 0143 loss_train: 0.6133 acc_train: 0.8429 loss_val: 0.8171 acc_val: 0.7867 time: 0.0687s\n",
      "Epoch: 0144 loss_train: 0.5527 acc_train: 0.9071 loss_val: 0.8150 acc_val: 0.7900 time: 0.0781s\n",
      "Epoch: 0145 loss_train: 0.5880 acc_train: 0.8357 loss_val: 0.8129 acc_val: 0.7900 time: 0.0855s\n",
      "Epoch: 0146 loss_train: 0.5690 acc_train: 0.9143 loss_val: 0.8108 acc_val: 0.7933 time: 0.0763s\n",
      "Epoch: 0147 loss_train: 0.5983 acc_train: 0.8929 loss_val: 0.8090 acc_val: 0.7967 time: 0.0781s\n",
      "Epoch: 0148 loss_train: 0.5621 acc_train: 0.8786 loss_val: 0.8072 acc_val: 0.7933 time: 0.0832s\n",
      "Epoch: 0149 loss_train: 0.5312 acc_train: 0.8929 loss_val: 0.8043 acc_val: 0.7933 time: 0.0717s\n",
      "Epoch: 0150 loss_train: 0.5741 acc_train: 0.9071 loss_val: 0.8023 acc_val: 0.7933 time: 0.0781s\n",
      "Epoch: 0151 loss_train: 0.5663 acc_train: 0.8643 loss_val: 0.7990 acc_val: 0.7933 time: 0.0828s\n",
      "Epoch: 0152 loss_train: 0.5654 acc_train: 0.8786 loss_val: 0.7974 acc_val: 0.7900 time: 0.0773s\n",
      "Epoch: 0153 loss_train: 0.5473 acc_train: 0.8929 loss_val: 0.7954 acc_val: 0.7900 time: 0.0781s\n",
      "Epoch: 0154 loss_train: 0.5335 acc_train: 0.9000 loss_val: 0.7930 acc_val: 0.7900 time: 0.0824s\n",
      "Epoch: 0155 loss_train: 0.5569 acc_train: 0.8929 loss_val: 0.7905 acc_val: 0.7900 time: 0.0747s\n",
      "Epoch: 0156 loss_train: 0.5142 acc_train: 0.8929 loss_val: 0.7871 acc_val: 0.7900 time: 0.0781s\n",
      "Epoch: 0157 loss_train: 0.5794 acc_train: 0.8571 loss_val: 0.7827 acc_val: 0.7933 time: 0.0807s\n",
      "Epoch: 0158 loss_train: 0.5220 acc_train: 0.9071 loss_val: 0.7786 acc_val: 0.7967 time: 0.0719s\n",
      "Epoch: 0159 loss_train: 0.5480 acc_train: 0.9000 loss_val: 0.7747 acc_val: 0.8067 time: 0.0781s\n",
      "Epoch: 0160 loss_train: 0.5504 acc_train: 0.9214 loss_val: 0.7718 acc_val: 0.8133 time: 0.0829s\n",
      "Epoch: 0161 loss_train: 0.5484 acc_train: 0.9286 loss_val: 0.7692 acc_val: 0.8133 time: 0.0649s\n",
      "Epoch: 0162 loss_train: 0.5518 acc_train: 0.8857 loss_val: 0.7669 acc_val: 0.8133 time: 0.0625s\n",
      "Epoch: 0163 loss_train: 0.5305 acc_train: 0.8929 loss_val: 0.7653 acc_val: 0.8033 time: 0.0882s\n",
      "Epoch: 0164 loss_train: 0.5584 acc_train: 0.9071 loss_val: 0.7652 acc_val: 0.8000 time: 0.0757s\n",
      "Epoch: 0165 loss_train: 0.5317 acc_train: 0.9357 loss_val: 0.7654 acc_val: 0.7967 time: 0.0781s\n",
      "Epoch: 0166 loss_train: 0.5293 acc_train: 0.8786 loss_val: 0.7655 acc_val: 0.7967 time: 0.0830s\n",
      "Epoch: 0167 loss_train: 0.4901 acc_train: 0.9214 loss_val: 0.7643 acc_val: 0.7933 time: 0.0773s\n",
      "Epoch: 0168 loss_train: 0.5470 acc_train: 0.8643 loss_val: 0.7626 acc_val: 0.7933 time: 0.0781s\n",
      "Epoch: 0169 loss_train: 0.5396 acc_train: 0.8857 loss_val: 0.7598 acc_val: 0.7933 time: 0.0811s\n",
      "Epoch: 0170 loss_train: 0.5220 acc_train: 0.9214 loss_val: 0.7555 acc_val: 0.8000 time: 0.0737s\n",
      "Epoch: 0171 loss_train: 0.5391 acc_train: 0.8929 loss_val: 0.7520 acc_val: 0.8000 time: 0.0781s\n",
      "Epoch: 0172 loss_train: 0.5021 acc_train: 0.9286 loss_val: 0.7494 acc_val: 0.8033 time: 0.0815s\n",
      "Epoch: 0173 loss_train: 0.4834 acc_train: 0.9143 loss_val: 0.7474 acc_val: 0.8033 time: 0.0750s\n",
      "Epoch: 0174 loss_train: 0.4906 acc_train: 0.9000 loss_val: 0.7465 acc_val: 0.8000 time: 0.0781s\n",
      "Epoch: 0175 loss_train: 0.5411 acc_train: 0.8643 loss_val: 0.7468 acc_val: 0.8000 time: 0.0890s\n",
      "Epoch: 0176 loss_train: 0.4816 acc_train: 0.9000 loss_val: 0.7470 acc_val: 0.7967 time: 0.0643s\n",
      "Epoch: 0177 loss_train: 0.4831 acc_train: 0.9143 loss_val: 0.7475 acc_val: 0.7933 time: 0.0781s\n",
      "Epoch: 0178 loss_train: 0.5147 acc_train: 0.8786 loss_val: 0.7463 acc_val: 0.7933 time: 0.0724s\n",
      "Epoch: 0179 loss_train: 0.4663 acc_train: 0.9214 loss_val: 0.7436 acc_val: 0.7933 time: 0.0750s\n",
      "Epoch: 0180 loss_train: 0.4401 acc_train: 0.9214 loss_val: 0.7410 acc_val: 0.7900 time: 0.0781s\n",
      "Epoch: 0181 loss_train: 0.4849 acc_train: 0.9143 loss_val: 0.7391 acc_val: 0.7933 time: 0.0872s\n",
      "Epoch: 0182 loss_train: 0.5093 acc_train: 0.9214 loss_val: 0.7369 acc_val: 0.7967 time: 0.0633s\n",
      "Epoch: 0183 loss_train: 0.5097 acc_train: 0.8929 loss_val: 0.7353 acc_val: 0.7967 time: 0.0781s\n",
      "Epoch: 0184 loss_train: 0.4983 acc_train: 0.9000 loss_val: 0.7340 acc_val: 0.8000 time: 0.0764s\n",
      "Epoch: 0185 loss_train: 0.4888 acc_train: 0.9143 loss_val: 0.7333 acc_val: 0.8000 time: 0.0617s\n",
      "Epoch: 0186 loss_train: 0.5063 acc_train: 0.9214 loss_val: 0.7321 acc_val: 0.8000 time: 0.0781s\n",
      "Epoch: 0187 loss_train: 0.4866 acc_train: 0.9357 loss_val: 0.7300 acc_val: 0.8000 time: 0.0806s\n",
      "Epoch: 0188 loss_train: 0.4718 acc_train: 0.8857 loss_val: 0.7279 acc_val: 0.8033 time: 0.0716s\n",
      "Epoch: 0189 loss_train: 0.4449 acc_train: 0.9071 loss_val: 0.7275 acc_val: 0.8033 time: 0.0781s\n",
      "Epoch: 0190 loss_train: 0.4414 acc_train: 0.9286 loss_val: 0.7280 acc_val: 0.8033 time: 0.0867s\n",
      "Epoch: 0191 loss_train: 0.4875 acc_train: 0.9357 loss_val: 0.7286 acc_val: 0.7967 time: 0.0783s\n",
      "Epoch: 0192 loss_train: 0.5003 acc_train: 0.9143 loss_val: 0.7285 acc_val: 0.7967 time: 0.0781s\n",
      "Epoch: 0193 loss_train: 0.4573 acc_train: 0.9143 loss_val: 0.7266 acc_val: 0.7967 time: 0.0805s\n",
      "Epoch: 0194 loss_train: 0.4319 acc_train: 0.9214 loss_val: 0.7247 acc_val: 0.7967 time: 0.0703s\n",
      "Epoch: 0195 loss_train: 0.4643 acc_train: 0.9214 loss_val: 0.7233 acc_val: 0.7967 time: 0.0781s\n",
      "Epoch: 0196 loss_train: 0.4298 acc_train: 0.9143 loss_val: 0.7217 acc_val: 0.8000 time: 0.0814s\n",
      "Epoch: 0197 loss_train: 0.4317 acc_train: 0.9500 loss_val: 0.7202 acc_val: 0.8000 time: 0.0739s\n",
      "Epoch: 0198 loss_train: 0.4512 acc_train: 0.9143 loss_val: 0.7192 acc_val: 0.8000 time: 0.0781s\n",
      "Epoch: 0199 loss_train: 0.4619 acc_train: 0.9000 loss_val: 0.7184 acc_val: 0.8000 time: 0.0884s\n",
      "Epoch: 0200 loss_train: 0.4546 acc_train: 0.9571 loss_val: 0.7174 acc_val: 0.8033 time: 0.0720s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 17.0793s\n",
      "Test set results: loss= 0.7729 accuracy= 0.8250\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils import load_data, accuracy\n",
    "from models import GCN\n",
    "\n",
    "# Training settings\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Disables CUDA training.')\n",
    "parser.add_argument('--fastmode', action='store_true', default=False,\n",
    "                    help='Validate during training pass.')\n",
    "parser.add_argument('--seed', type=int, default=42, help='Random seed.')\n",
    "parser.add_argument('--epochs', type=int, default=200,\n",
    "                    help='Number of epochs to train.')\n",
    "parser.add_argument('--lr', type=float, default=0.01,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
    "                    help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--hidden', type=int, default=16,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "\n",
    "#args = parser.parse_args()\n",
    "#error midad khate bala,khate paein jaigozin shod\n",
    "\n",
    "args = parser.parse_known_args()[0]\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# Load data\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()\n",
    "\n",
    "# Model and optimizer\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=args.hidden,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=args.dropout)\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if not args.fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "\n",
    "\n",
    "# Train model\n",
    "t_total = time.time()\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Testing\n",
    "test()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
